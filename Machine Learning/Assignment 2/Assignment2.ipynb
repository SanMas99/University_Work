{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Sanad Masannat\n",
    "\n",
    "ID: 24217734\n",
    "\n",
    "Assignment 2 Machine Learning with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sanad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sanad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\sanad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sanad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sanad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase and remove punctuation\n",
    "    text = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Tokenize words\n",
    "    words = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we aim to get a list of all the common English stop_words, and preprocess the text by making it all lowercase, removing punctuation , tokenise the words, then removing all stop words and then lemmatize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "      <th>Processed_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hariri killing hits Beirut shares  Shares in S...</td>\n",
       "      <td>business</td>\n",
       "      <td>hariri killing hit beirut share share solidere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Asian banks halt dollar's slide  The dollar re...</td>\n",
       "      <td>business</td>\n",
       "      <td>asian bank halt dollar slide dollar regained l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Housewives lift Channel 4 ratings  The debut o...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>housewife lift channel 4 rating debut u televi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Portable PlayStation ready to go  Sony's PlayS...</td>\n",
       "      <td>tech</td>\n",
       "      <td>portable playstation ready go sonys playstatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Georgia plans hidden asset pardon  Georgia is ...</td>\n",
       "      <td>business</td>\n",
       "      <td>georgia plan hidden asset pardon georgia offer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text          Class  \\\n",
       "0  Hariri killing hits Beirut shares  Shares in S...       business   \n",
       "1  Asian banks halt dollar's slide  The dollar re...       business   \n",
       "2  Housewives lift Channel 4 ratings  The debut o...  entertainment   \n",
       "3  Portable PlayStation ready to go  Sony's PlayS...           tech   \n",
       "4  Georgia plans hidden asset pardon  Georgia is ...       business   \n",
       "\n",
       "                                      Processed_Text  \n",
       "0  hariri killing hit beirut share share solidere...  \n",
       "1  asian bank halt dollar slide dollar regained l...  \n",
       "2  housewife lift channel 4 rating debut u televi...  \n",
       "3  portable playstation ready go sonys playstatio...  \n",
       "4  georgia plan hidden asset pardon georgia offer...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"BBC_news.csv\")\n",
    "\n",
    "# Apply the preprocessing done above to the text\n",
    "df[\"Processed_Text\"] = df[\"Text\"].apply(preprocess_text)\n",
    "\n",
    "#Make sure all columns and preprocessing is done correctly\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the csv file and apply the preprocessing we defined earlier to it, saving it to a new column. we then display a few records to make sure it is in fact done correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9520958083832335\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     business       0.93      0.97      0.95        38\n",
      "entertainment       1.00      0.94      0.97        31\n",
      "     politics       0.93      0.97      0.95        29\n",
      "        sport       0.93      1.00      0.96        40\n",
      "         tech       1.00      0.86      0.93        29\n",
      "\n",
      "     accuracy                           0.95       167\n",
      "    macro avg       0.96      0.95      0.95       167\n",
      " weighted avg       0.95      0.95      0.95       167\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "model = LogisticRegression()\n",
    "X = vectorizer.fit_transform(df[\"Processed_Text\"])  # Transform text into numerical features\n",
    "y = df[\"Class\"]  # Target labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(\"Classification Report:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use a standard Logistic Regression model on the data. We use a 70/30 split of test and training here then fit the model to the training data. We then use our test values to predict the Y values and then compare them and get an accuracy score\n",
    "\n",
    "**Findings**\n",
    "\n",
    "Our findings here is that we get an decent accuracy score of 95.2%. If we go through each possible category, we can see tech and entertainment have the highest precision but tech has the lowest recall value of .86 while entertainment has the second lowest one with .94. Using entertainment as a refernce, this means it misclassifies a few articles as non-entertainment when it was but its prediction rate is high. Conversely Sports has the lowest precision but the highest recall, meaning if it predicts all sports articles were correctly classified but the prescision indicts in classified non-sports articles as sports. The F1-scores range is from 0.93 to 0.97. This means it was not difficult for the model to classify each text accordingly to a class as F1 scores aim to balance both precision and recall so if one struggled with one metric but did better in the other, the f1 score adjusts accordingly. This model appeared t struggle the most with tech articles as despite the high precision rate, it has the lowest f1-score and a really low recall compared to the other classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec Accuracy: 0.9401\n",
      "Doc2Vec Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     business       0.90      0.97      0.94        38\n",
      "entertainment       1.00      0.94      0.97        31\n",
      "     politics       0.93      0.93      0.93        29\n",
      "        sport       0.97      0.95      0.96        40\n",
      "         tech       0.90      0.90      0.90        29\n",
      "\n",
      "     accuracy                           0.94       167\n",
      "    macro avg       0.94      0.94      0.94       167\n",
      " weighted avg       0.94      0.94      0.94       167\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Prepare data for Doc2Vec as each document will need an associated tag\n",
    "tagged_data = [TaggedDocument(words=text.split(), tags=[str(i)]) for i, text in enumerate(df[\"Processed_Text\"])]\n",
    "\n",
    "# Define and train the Doc2Vec model\n",
    "doc2vec_model = Doc2Vec(vector_size=100, window=5, min_count=2, workers=4, epochs=20)\n",
    "doc2vec_model.build_vocab(tagged_data)\n",
    "doc2vec_model.train(tagged_data, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "\n",
    "# Convert text data into embeddings\n",
    "X_doc2vec = np.array([doc2vec_model.infer_vector(text.split()) for text in df[\"Processed_Text\"]])\n",
    "\n",
    "# Encode class labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(df[\"Class\"])\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_doc2vec, y_encoded, test_size=0.3,random_state=42)\n",
    "\n",
    "# Train Logistic Regression\n",
    "model_doc2vec = LogisticRegression()\n",
    "model_doc2vec.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_doc2vec = model_doc2vec.predict(X_test)\n",
    "\n",
    "# Print results\n",
    "accuracy_doc2vec = accuracy_score(y_test, y_pred_doc2vec)\n",
    "classification_rep_doc2vec = classification_report(y_test, y_pred_doc2vec, target_names=label_encoder.classes_)\n",
    "\n",
    "print(f\"Doc2Vec Accuracy: {accuracy_doc2vec:.4f}\")\n",
    "print(\"Doc2Vec Classification Report:\\n\", classification_rep_doc2vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use a  Logistic Regression model on the data but this time, we use gensim's doc2vec to produce embeddings for the model. After we prepare the data for the gensim model, we change the data so the model is able to accept them and then wee encode the data.After that, we train the model and then run We use a 70/30 split of test and training here then fit the model to the training data. We then use our test values to predict the Y values and then compare them and get an accuracy score\n",
    "\n",
    "Findings\n",
    "\n",
    "Our findings here is that we get a lower accuracy score of 94%. If we go through each possible category, we can see entertainment once again has the highest precision and the median recall value, meaning it misclassifies a few articles entertainment articles as non-entertainment but its prediction rate is high. Here buisness and tech have the lowest prediction values of 0.9 each/ This time, tech has the lowest recall of 0.9 whereas business has the highest recall of 0.97. The F1-scores of each class is a bit more spread out indicated by a wider range (0.9 to 0.97) The model appered to struggle with tech articles the most (indicated with the lowest recall and f1 score of 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Accuracy: 0.9521\n",
      "BERT Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     business       0.95      0.92      0.93        38\n",
      "entertainment       1.00      0.97      0.98        31\n",
      "     politics       0.93      0.97      0.95        29\n",
      "        sport       1.00      1.00      1.00        40\n",
      "         tech       0.87      0.90      0.88        29\n",
      "\n",
      "     accuracy                           0.95       167\n",
      "    macro avg       0.95      0.95      0.95       167\n",
      " weighted avg       0.95      0.95      0.95       167\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to convert text into BERT embeddings\n",
    "def get_bert_embedding(text):\n",
    "    tokens = tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "        output = bert_model(**tokens)\n",
    "    return output.last_hidden_state.mean(dim=1).squeeze().numpy()  # Mean pooling\n",
    "\n",
    "# Convert dataset text into BERT embeddings\n",
    "X_bert = np.array([get_bert_embedding(text) for text in df[\"Processed_Text\"]])\n",
    "\n",
    "# Encode class labels and set up the model\n",
    "label_encoder = LabelEncoder()\n",
    "model_bert = LogisticRegression()\n",
    "\n",
    "y_encoded = label_encoder.fit_transform(df[\"Class\"])\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bert, y_encoded, test_size=0.3,random_state=42)\n",
    "\n",
    "# Train the model and make predictions\n",
    "\n",
    "model_bert.fit(X_train, y_train)\n",
    "y_pred_bert = model_bert.predict(X_test)\n",
    "\n",
    "# Print results\n",
    "accuracy_bert = accuracy_score(y_test, y_pred_bert)\n",
    "classification_rep_bert = classification_report(y_test, y_pred_bert, target_names=label_encoder.classes_)\n",
    "\n",
    "print(f\"BERT Accuracy: {accuracy_bert:.4f}\")\n",
    "print(\"BERT Classification Report:\\n\", classification_rep_bert)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use once again use a Logsitic Regression model on the data but this time, we use Bert to produce embeddings for the model. After we tokenise the data and convert the text data into embeddings, we train the model and then run We use a 70/30 split of test and training here then fit the model to the training data. We then use our test values to predict the Y values and then compare them and get an accuracy score\n",
    "\n",
    "Findings\n",
    "\n",
    "Our findings here is that we the highest accuracy score of 95.21%. If we go through each possible category, we can see that  Sport articles had a perfect score for precision, recall and f1-score. Entertainment had an equal precision but a lower recall. Once again, tech has the lowest precision, recall and f1-score at 0.87, 0.8 and 0.88 respectively each metric. Entertainment articles has a really high f1-score that is near perfect but due to a recall value of less than 1 led to a lower f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Thoughts**\n",
    "\n",
    "Based of all the findings, we can see that the BERT model performed the best, followed by the baseline model which was then followed by the doc2vec  model. The BERT model and the baseline model were close to each other in accuracy with just a difference in 0.1%. The doc2vec model had a lower accuracy of about 1% in comparison to both. The baseline model seemed to struggle the most with tech articles but performed well with entertainment articles. The Doc2Vec model did well classifying entertainment articles but struggled with tech the most. Finally, BERT did really well with sport articles but did bad with tech articles once again.\n",
    "\n",
    "It appeared all three models did bad with tech articles, this could be due to the fact that technology is used in different sectors so words from tech articles could be used in other different article types. Initially it could be due to the fact that tech articles are the lowest in count but the models performed considerable better in politics articles when it has the same number of articles.\n",
    "\n",
    "To ensure we the models operate correctly, we first made sure we use the same data, the same test/train split function by keeping the random state the same and the same Logistic regression models. We also used the same metrics for measuring accuracy and classification scores to make sure nothing is different. The reason this occurs is if we dont set a random state, the ranking slightly changes and the accuracy will change as well, in preliminary testing, BERT had a accuracy at around 97 and the doc2vec model produced a better accuracy than the baseline but after setting an equal random state for all models, we achieved the current ranking of BERT>Baseline>Doc2Vec.\n",
    "\n",
    "To ensure that all the models are working as intended: we did the following:\n",
    "\n",
    "    1. We tokenised and converted the data correctly for the BERT model\n",
    "\n",
    "    2. We made sure to use Logistic Regression on top the of the different embeddings to make sure comparisons are fair\n",
    "\n",
    "    3. We tagged and generated embeddings prior to using our Logistic Regression Models\n",
    "\n",
    "    4. We set the random state for all models to be equal\n",
    "\n",
    "    5. We removed the maximum iterations ChatGPT gave us to let the models be slightly more accurate\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
