{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests \n",
    "import urllib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import all the necessary tools so that the necessary dependancies are globally declared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(start_time,end_time,min_magnitude=3):\n",
    "    #Here we write down the URL of the chosen API and get the format ready for data collection\n",
    "    api_url = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
    "\n",
    "    #Below are the chosen parameters which will be used for the visualisation of data, cleaning will be done later\n",
    "    params = {\n",
    "    \"format\": \"geojson\",\n",
    "    \"starttime\": start_time,\n",
    "    \"endtime\": end_time,\n",
    "    \"minmagnitude\": min_magnitude,\n",
    "\n",
    "    }\n",
    "    #Attempt to pull from the API using the chosen paramteres\n",
    "    api_data = requests.get(api_url, params=params)\n",
    "    #Create an empty dataframe\n",
    "    earthquake_data=pd.DataFrame()   \n",
    "    # Check if the request was successful and if so, begin collection\n",
    "    if api_data.status_code == 200:\n",
    "        data = api_data.json()\n",
    "        # Extract earthquake features\n",
    "        features = data['features']\n",
    "\n",
    "        \n",
    "        \n",
    "        # Parse the data and store in a DataFrame\n",
    "        earthquake_list = []\n",
    "        for feature in features:\n",
    "            properties = feature['properties']\n",
    "            geometry = feature['geometry']\n",
    "            #Rather than making a list and continuously apppending to it then converting to a dataframe, we concat the current\n",
    "            #dataframe we have with a newly created one (one created by iterating through all features we want)\n",
    "            earthquake_data=pd.concat([earthquake_data, pd.DataFrame([{\n",
    "                \"eventid\": feature['id'],\n",
    "                \"time\": properties['time'],\n",
    "                \"latitude\": geometry['coordinates'][1],\n",
    "                \"longitude\": geometry['coordinates'][0],\n",
    "                \"location\": properties['place'],\n",
    "                \"depth\": geometry['coordinates'][2],\n",
    "                \"magnitude\": properties['mag'],\n",
    "                \n",
    "            }])]) \n",
    "\n",
    "        return earthquake_data\n",
    "\n",
    "    else:\n",
    "        print(\"Error, there is an issue with data retrieval, error code, \", api_data.status_code)\n",
    "        return earthquake_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above function, we define the paramters which will be use to call the AP. In this case we use the minimum magnitude, start and end time (These paramters determine the range in which we pull the data from), and the format which is used. Afterwards, we attempt to pull data from the API and if successful, we extract the required features from it and then save it to a Database, later to be converted to a CSV file. here we constantly append to the database rather than make a list and then append to it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collection complete. Dataset saved as  raw_earthquake_data.csv to path  d:\\University_Work\\Data Science\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eventid</th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>location</th>\n",
       "      <th>depth</th>\n",
       "      <th>magnitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usp0009vf6</td>\n",
       "      <td>962320972290</td>\n",
       "      <td>29.796000</td>\n",
       "      <td>-42.880000</td>\n",
       "      <td>northern Mid-Atlantic Ridge</td>\n",
       "      <td>10.000</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usp0009vf5</td>\n",
       "      <td>962320920420</td>\n",
       "      <td>6.382000</td>\n",
       "      <td>-72.804000</td>\n",
       "      <td>4 km NNE of Onzaga, Colombia</td>\n",
       "      <td>33.000</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usp0009vf4</td>\n",
       "      <td>962320856800</td>\n",
       "      <td>-30.232000</td>\n",
       "      <td>-71.530000</td>\n",
       "      <td>35 km SSW of Coquimbo, Chile</td>\n",
       "      <td>51.000</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usp0009vf3</td>\n",
       "      <td>962312599430</td>\n",
       "      <td>-55.758000</td>\n",
       "      <td>-28.470000</td>\n",
       "      <td>South Sandwich Islands region</td>\n",
       "      <td>33.000</td>\n",
       "      <td>4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usp0009vf0</td>\n",
       "      <td>962310160220</td>\n",
       "      <td>7.617000</td>\n",
       "      <td>-37.095000</td>\n",
       "      <td>central Mid-Atlantic Ridge</td>\n",
       "      <td>10.000</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uw10524793</td>\n",
       "      <td>962306846760</td>\n",
       "      <td>48.466833</td>\n",
       "      <td>-123.104167</td>\n",
       "      <td>9 km SW of Friday Harbor, Washington</td>\n",
       "      <td>27.204</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usp0009vex</td>\n",
       "      <td>962304843940</td>\n",
       "      <td>33.997000</td>\n",
       "      <td>139.350000</td>\n",
       "      <td>84 km SSE of Shimoda, Japan</td>\n",
       "      <td>10.000</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usp0009vew</td>\n",
       "      <td>962300683320</td>\n",
       "      <td>36.346000</td>\n",
       "      <td>138.669000</td>\n",
       "      <td>20 km W of Annaka, Japan</td>\n",
       "      <td>165.800</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usp0009veu</td>\n",
       "      <td>962300277980</td>\n",
       "      <td>13.523000</td>\n",
       "      <td>144.040000</td>\n",
       "      <td>68 km WNW of Agat Village, Guam</td>\n",
       "      <td>33.000</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usp0009vet</td>\n",
       "      <td>962299947700</td>\n",
       "      <td>33.920000</td>\n",
       "      <td>139.460000</td>\n",
       "      <td>96 km SSE of Shimoda, Japan</td>\n",
       "      <td>10.000</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      eventid          time   latitude   longitude  \\\n",
       "0  usp0009vf6  962320972290  29.796000  -42.880000   \n",
       "0  usp0009vf5  962320920420   6.382000  -72.804000   \n",
       "0  usp0009vf4  962320856800 -30.232000  -71.530000   \n",
       "0  usp0009vf3  962312599430 -55.758000  -28.470000   \n",
       "0  usp0009vf0  962310160220   7.617000  -37.095000   \n",
       "0  uw10524793  962306846760  48.466833 -123.104167   \n",
       "0  usp0009vex  962304843940  33.997000  139.350000   \n",
       "0  usp0009vew  962300683320  36.346000  138.669000   \n",
       "0  usp0009veu  962300277980  13.523000  144.040000   \n",
       "0  usp0009vet  962299947700  33.920000  139.460000   \n",
       "\n",
       "                               location    depth  magnitude  \n",
       "0           northern Mid-Atlantic Ridge   10.000        4.5  \n",
       "0          4 km NNE of Onzaga, Colombia   33.000        4.3  \n",
       "0          35 km SSW of Coquimbo, Chile   51.000        4.3  \n",
       "0         South Sandwich Islands region   33.000        4.9  \n",
       "0            central Mid-Atlantic Ridge   10.000        4.6  \n",
       "0  9 km SW of Friday Harbor, Washington   27.204        3.5  \n",
       "0           84 km SSE of Shimoda, Japan   10.000        4.4  \n",
       "0              20 km W of Annaka, Japan  165.800        4.4  \n",
       "0       68 km WNW of Agat Village, Guam   33.000        4.5  \n",
       "0           96 km SSE of Shimoda, Japan   10.000        4.1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_earthquake_data=pd.DataFrame()\n",
    "\n",
    "#Here we loop through the last 24 years, one by one in two 6-month blocks, to prevent any errors when retrieving from too large of data\n",
    "for i in range(2000,2024):\n",
    "    start=str(i)+(\"-01-01\")\n",
    "    end=str(i)+(\"-6-30\")\n",
    "    partial_earthquake_data=collect_data(start,end)\n",
    "    full_earthquake_data = pd.concat([full_earthquake_data,partial_earthquake_data])\n",
    "    start=str(i)+(\"-07-01\")\n",
    "    end=str(i)+(\"-12-30\")\n",
    "    partial_earthquake_data=collect_data(start,end)\n",
    "    full_earthquake_data = pd.concat([full_earthquake_data,partial_earthquake_data])\n",
    "\n",
    "#As the API collects data till now, we want to add any data collected till now\n",
    "partial_earthquake_data=collect_data(\"2024-01-01\",datetime.today().strftime('%Y-%m-%d'))\n",
    "full_earthquake_data = pd.concat([full_earthquake_data,partial_earthquake_data])\n",
    "\n",
    "# Save the dataset to a CSV file\n",
    "csv_file = \"raw_earthquake_data.csv\"\n",
    "full_earthquake_data.to_csv(csv_file, index=False)\n",
    "\n",
    "# Output message\n",
    "print(f\"Data collection complete. Dataset saved as \", csv_file, \"to path \", Path.cwd())\n",
    "\n",
    "full_earthquake_data.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over here, we create the data frame which will be used to store the data we pulled from the API\n",
    "\n",
    "To prevent any errors when collecting data fom the API, we collect the data in two half-year blocks per year in a single loop and filter using a pre-set minimum magnitude of 3. Since an error will occur if we try to pull too much data at once, this is done to circumvent this and get a lot of data to have more accurate graphs for the second half of the assignment. Hoewver as data is still being added to it til lthis day, we also pull data from the API till this day to make sure we don't miss any data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
