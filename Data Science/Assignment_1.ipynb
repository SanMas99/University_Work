{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests \n",
    "import urllib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import all the necessary tools so that the necessary dependancies are globaly declared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(start_time,end_time,min_magnitude=5):\n",
    "    #Here we write down the URL of the chosen API and get the format ready for data collection\n",
    "    api_url = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
    "\n",
    "    #Below are the chosen parameters which will be used for the visualisation of data, cleaning will be done later\n",
    "    params = {\n",
    "    \"format\": \"geojson\",\n",
    "    \"starttime\": start_time,\n",
    "    \"endtime\": end_time,\n",
    "    \"minmagnitude\": min_magnitude,\n",
    "\n",
    "    }\n",
    "    #Attempt to pull from the API using the chosen paramteres\n",
    "    api_data = requests.get(api_url, params=params)\n",
    "    #Create an aempty dataframe\n",
    "    earthquake_data=pd.DataFrame()   \n",
    "    # Check if the request was successful and if so, begin collection\n",
    "    if api_data.status_code == 200:\n",
    "        data = api_data.json()\n",
    "        # Extract earthquake features\n",
    "        features = data['features']\n",
    "\n",
    "        \n",
    "        \n",
    "        # Parse the data and store in a DataFrame\n",
    "        earthquake_list = []\n",
    "        for feature in features:\n",
    "            properties = feature['properties']\n",
    "            geometry = feature['geometry']\n",
    "            #Rather than making a list and continuously apppending to it then converting to a dataframe, we concat the current\n",
    "            # dataframe we have with a newly created one (one created by iterating through all features we want)\n",
    "            earthquake_data=pd.concat([earthquake_data, pd.DataFrame([{\n",
    "                \"eventid\": feature['id'],\n",
    "                \"time\": properties['time'],\n",
    "                \"latitude\": geometry['coordinates'][1],\n",
    "                \"longitude\": geometry['coordinates'][0],\n",
    "                \"country\": properties['place'],\n",
    "                \"depth\": geometry['coordinates'][2],\n",
    "                \"magnitude\": properties['mag'],\n",
    "                \n",
    "            }])]) \n",
    "\n",
    "        return earthquake_data\n",
    "\n",
    "    else:\n",
    "        print(\"Error, there is an issue with data rretrieval, error code, \", api_data.status_code)\n",
    "        return earthquake_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collection complete. Dataset saved as  raw_earthquake_data.csv to path  d:\\University_Work\\Data Science\n"
     ]
    }
   ],
   "source": [
    "full_earthquake_data=pd.DataFrame()\n",
    "\n",
    "#Here we loop through the last 24 years, one by one, to prevent any errors from too large of data\n",
    "for i in range(2000,2024):\n",
    "    start=str(i)+(\"-01-01\")\n",
    "    end=str(i)+(\"-12-31\")\n",
    "    partial_earthquake_data=collect_data(start,end)\n",
    "    full_earthquake_data = pd.concat([full_earthquake_data,partial_earthquake_data])\n",
    "\n",
    "#As the API collects dta till now, we want to add any data collected till now\n",
    "partial_earthquake_data=collect_data(\"2024-01-01\",datetime.today().strftime('%Y-%m-%d'))\n",
    "full_earthquake_data = pd.concat([full_earthquake_data,partial_earthquake_data])\n",
    "\n",
    "# Save the dataset to a CSV file\n",
    "csv_file = \"raw_earthquake_data.csv\"\n",
    "full_earthquake_data.to_csv(csv_file, index=False)\n",
    "\n",
    "# Output message\n",
    "print(f\"Data collection complete. Dataset saved as \", csv_file, \"to path \", Path.cwd())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
